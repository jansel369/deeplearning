
import torch as pt
from backend import gradient as g


""" Activation forward
"""



""" Runs all forward propagations
"""


"""
     Back prop
"""

""" gradients
"""

# def liniar_grad(activation_backward):
#     def f(dA, A):
#         return dA * activation_backward(A)

#     return f

# def activation_grad(W, dZ):
#     return W.t().mm(dZ)








